{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development data shape: (50000, 35)\n",
      "Test data shape: (5000, 35)\n",
      "Columns in development data: ['ID', 'customer_id', 'application_date', 'target', 'Application_status', 'Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var8', 'Var9', 'Var10', 'Var11', 'Var12', 'Var13', 'Var14', 'Var15', 'Var16', 'Var17', 'Var18', 'Var19', 'Var20', 'Var21', 'Var22', 'Var23', 'Var24', 'Var25', 'Var26', 'Var27', 'Var28', 'Var29', 'Var30']\n",
      "         ID  customer_id   application_date  target Application_status  Var1  \\\n",
      "0  11034977     32537148  01Feb2010 0:00:00     0.0           Approved     1   \n",
      "1  11034978     32761663  01Feb2010 0:00:00     0.0           Approved     1   \n",
      "2  11034979     32701063  01Feb2010 0:00:00     0.0           Approved     2   \n",
      "3  11034980     32386786  01Feb2010 0:00:00     0.0           Approved     3   \n",
      "4  11034981     32692110  02Feb2010 0:00:00     NaN           Rejected     1   \n",
      "\n",
      "   Var2 Var3   Var4  Var5  ...  Var21  Var22  Var23  Var24    Var25     Var26  \\\n",
      "0   2.0    1   7800    99  ...      1      1      1      0  6768.42      0.00   \n",
      "1   1.0    2  11100    78  ...      0      0      0  15000  5937.66   3870.60   \n",
      "2   3.0    1   2400    15  ...      2      2      2      0  5647.77   1463.08   \n",
      "3   1.0    2  11800    30  ...      0      0      0      0  3594.90  21563.78   \n",
      "4   2.0    1  10200    72  ...      0      2      2      0  9908.45      0.00   \n",
      "\n",
      "   Var27 Var28  Var29  Var30  \n",
      "0      0     0      0   3899  \n",
      "1      0     0      0   3899  \n",
      "2      0     0     10   3899  \n",
      "3      0     0     10   3899  \n",
      "4      0     0     20   3899  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "Target distribution:\n",
      "target\n",
      "0.0    35591\n",
      "NaN    13282\n",
      "1.0     1127\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "dev_df = pd.read_csv('development_sample.csv')\n",
    "test_df = pd.read_csv('testing_sample.csv')\n",
    "var_desc = pd.read_excel('Variables_description.xlsx')\n",
    "\n",
    "# Quick look at shapes\n",
    "print(f\"Development data shape: {dev_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Check column names\n",
    "print(\"Columns in development data:\", dev_df.columns.tolist())\n",
    "\n",
    "# Check first few rows\n",
    "print(dev_df.head())\n",
    "\n",
    "# Check target balance\n",
    "print(\"Target distribution:\")\n",
    "print(dev_df['target'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n",
      "ID                      int64\n",
      "customer_id             int64\n",
      "application_date       object\n",
      "target                float64\n",
      "Application_status     object\n",
      "Var1                    int64\n",
      "Var2                  float64\n",
      "Var3                   object\n",
      "Var4                    int64\n",
      "Var5                    int64\n",
      "Var6                    int64\n",
      "Var7                  float64\n",
      "Var8                  float64\n",
      "Var9                    int64\n",
      "Var10                 float64\n",
      "Var11                   int64\n",
      "Var12                 float64\n",
      "Var13                  object\n",
      "Var14                   int64\n",
      "Var15                   int64\n",
      "Var16                   int64\n",
      "Var17                 float64\n",
      "Var18                 float64\n",
      "Var19                 float64\n",
      "Var20                   int64\n",
      "Var21                   int64\n",
      "Var22                   int64\n",
      "Var23                   int64\n",
      "Var24                   int64\n",
      "Var25                 float64\n",
      "Var26                 float64\n",
      "Var27                   int64\n",
      "Var28                   int64\n",
      "Var29                   int64\n",
      "Var30                   int64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "ID                        0\n",
      "customer_id               0\n",
      "application_date          0\n",
      "target                13282\n",
      "Application_status        0\n",
      "Var1                      0\n",
      "Var2                   1403\n",
      "Var3                   1403\n",
      "Var4                      0\n",
      "Var5                      0\n",
      "Var6                      0\n",
      "Var7                      0\n",
      "Var8                  29173\n",
      "Var9                      0\n",
      "Var10                 37538\n",
      "Var11                     0\n",
      "Var12                 37538\n",
      "Var13                     0\n",
      "Var14                     0\n",
      "Var15                     0\n",
      "Var16                     0\n",
      "Var17                    40\n",
      "Var18                 37416\n",
      "Var19                 29173\n",
      "Var20                     0\n",
      "Var21                     0\n",
      "Var22                     0\n",
      "Var23                     0\n",
      "Var24                     0\n",
      "Var25                 10088\n",
      "Var26                 19883\n",
      "Var27                     0\n",
      "Var28                     0\n",
      "Var29                     0\n",
      "Var30                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types and missing values\n",
    "print(\"\\nData types:\")\n",
    "print(dev_df.dtypes)\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(dev_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted date features.\n"
     ]
    }
   ],
   "source": [
    "# Convert application_date to datetime\n",
    "dev_df['application_date'] = pd.to_datetime(dev_df['application_date'], errors='coerce')\n",
    "test_df['application_date'] = pd.to_datetime(test_df['application_date'], errors='coerce')\n",
    "\n",
    "# Create new date features\n",
    "for df in [dev_df, test_df]:\n",
    "    df['application_year'] = df['application_date'].dt.year\n",
    "    df['application_month'] = df['application_date'].dt.month\n",
    "    df['application_dayofweek'] = df['application_date'].dt.dayofweek\n",
    "\n",
    "print(\"Extracted date features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['Application_status', 'Var3', 'Var13']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "cat_cols = dev_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(\"Categorical columns:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    dev_df[col] = dev_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = ['Var8', 'Var10', 'Var12', 'Var18', 'Var19']\n",
    "\n",
    "for col in missing_cols:\n",
    "    dev_df[col + '_missing'] = dev_df[col].isnull().astype(int)\n",
    "    test_df[col + '_missing'] = test_df[col].isnull().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in missing_cols:\n",
    "    median_val = dev_df[col].median()\n",
    "    dev_df[col] = dev_df[col].fillna(median_val)\n",
    "    test_df[col] = test_df[col].fillna(median_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['ID', 'customer_id', 'application_date']  \n",
    "\n",
    "dev_df = dev_df.drop(columns=[col for col in drop_cols if col in dev_df.columns])\n",
    "test_df = test_df.drop(columns=[col for col in drop_cols if col in test_df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: '0'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/_encode.py:235\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/_encode.py:174\u001b[39m, in \u001b[36m_map_to_integer\u001b[39m\u001b[34m(values, uniques)\u001b[39m\n\u001b[32m    173\u001b[39m table = _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device=device(values))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/_encode.py:167\u001b[39m, in \u001b[36m_nandict.__missing__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nan_value\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: '0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     le.fit(combined_data)\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Transform both sets - convert to string to avoid type mismatch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     dev_df[col] = \u001b[43mle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     test_df[col] = le.transform(test_df[col].astype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCategorical columns label-encoded successfully (forcing string dtype).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_label.py:134\u001b[39m, in \u001b[36mLabelEncoder.transform\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) == \u001b[32m0\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([])\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/_encode.py:237\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    235\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[31mValueError\u001b[39m: y contains previously unseen labels: '0'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Combine data as STRING to capture all unique values\n",
    "    combined_data = pd.concat([dev_df[col], test_df[col]], axis=0).astype(str)\n",
    "    \n",
    "    # Fit on the combined data\n",
    "    le.fit(combined_data)\n",
    "    \n",
    "    # Transform both sets - convert to string to avoid type mismatch\n",
    "    dev_df[col] = le.transform(dev_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "\n",
    "print(\"Categorical columns label-encoded successfully (forcing string dtype).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train uniques for Var3: [0 1 5 3 2 4]\n",
      "Test uniques for Var3: [1.0, 2.0, NaN, 3.0]\n",
      "Categories (3, float64): [1.0, 2.0, 3.0]\n",
      "Encoder classes: ['0.0' '1.0' '2.0' '3.0' '4.0' '5.0' 'nan']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train uniques for {col}: {dev_df[col].unique()}\")\n",
    "print(f\"Test uniques for {col}: {test_df[col].unique()}\")\n",
    "print(f\"Encoder classes: {le.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns being processed: ['Application_status', 'Var3', 'Var13']\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical columns being processed:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Var13 (sample):\n",
      "['26Nov2004', '16Sep1996', '06May1989', '06Dec1987', '18May1989', ..., '15Oct1987', '27Oct2007', '21Oct2002', '23Jan2009', '03Feb2002']\n",
      "Length: 20\n",
      "Categories (10934, object): ['01Apr1989', '01Apr1990', '01Apr1991', '01Apr1992', ..., '31Oct2015', '31Oct2016', '31Oct2017', '31Oct2018']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in Var13 (sample):\")\n",
    "print(dev_df['Var13'].dropna().unique()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Var13 (sample - test):\n",
      "['28Nov2004', '19Sep1996', '23Nov1998', '13Nov2005', '06Aug1995', ..., '08Jun1991', '23Sep2000', '04Dec2002', '08Mar2009', '31Dec9999']\n",
      "Length: 20\n",
      "Categories (3912, object): ['01Apr1995', '01Apr1999', '01Apr2002', '01Apr2004', ..., '31Oct2003', '31Oct2004', '31Oct2006', '31Oct2008']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in Var13 (sample - test):\")\n",
    "print(test_df['Var13'].dropna().unique()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final categorical columns to encode: ['Application_status', 'Var3']\n"
     ]
    }
   ],
   "source": [
    "cat_cols = ['Application_status', 'Var3']  # Removed Var13 ✅\n",
    "print(\"Final categorical columns to encode:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixing issue in var_3\n",
    "(dev_df) Var3 has integers [0, 1, 2, 3, 4, 5] \n",
    "(test_df) Var3 has float + NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns label-encoded successfully (excluding Var13).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    combined_data = pd.concat([dev_df[col], test_df[col]], axis=0).astype(float).astype(str)\n",
    "    \n",
    "    le.fit(combined_data)\n",
    "    dev_df[col] = le.transform(dev_df[col].astype(float).astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(float).astype(str))\n",
    "\n",
    "print(\"Categorical columns label-encoded successfully (excluding Var13).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Var13 to datetime\n",
    "# dev_df['Var13_date'] = pd.to_datetime(dev_df['Var13'], format='%d%b%Y', errors='coerce')\n",
    "# test_df['Var13_date'] = pd.to_datetime(test_df['Var13'], format='%d%b%Y', errors='coerce')\n",
    "\n",
    "# # Extract year, month (or compute age if it's DOB)\n",
    "# dev_df['Var13_year'] = dev_df['Var13_date'].dt.year\n",
    "# dev_df['Var13_month'] = dev_df['Var13_date'].dt.month\n",
    "\n",
    "# test_df['Var13_year'] = test_df['Var13_date'].dt.year\n",
    "# test_df['Var13_month'] = test_df['Var13_date'].dt.month\n",
    "\n",
    "#later decide if need to include the code for dropping var 13 or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns label-encoded successfully (excluding Var13).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    combined_data = pd.concat([dev_df[col], test_df[col]], axis=0).astype(float).astype(str)\n",
    "    \n",
    "    le.fit(combined_data)\n",
    "    dev_df[col] = le.transform(dev_df[col].astype(float).astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(float).astype(str))\n",
    "\n",
    "print(\"Categorical columns label-encoded successfully (excluding Var13).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop NaN Targets Before Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping NaN targets, data shape: (36718, 42)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where target is NaN (for supervised learning)\n",
    "clean_data = dev_df.dropna(subset=['target'])\n",
    "\n",
    "# Separate features and target\n",
    "X = clean_data.drop(columns=['target'])\n",
    "y = clean_data['target']\n",
    "\n",
    "print(\"After dropping NaN targets, data shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (29374, 42)\n",
      "Validation shape: (7344, 42)\n",
      "Target distribution in train:\n",
      " target\n",
      "0.0    0.969293\n",
      "1.0    0.030707\n",
      "Name: proportion, dtype: float64\n",
      "Target distribution in validation:\n",
      " target\n",
      "0.0    0.969363\n",
      "1.0    0.030637\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split: 80% train, 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Target distribution in train:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Target distribution in validation:\\n\", y_val.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '05Jan2017'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/y8/phsgwx2n6rxfxd31jgj0f14m0000gn/T/ipykernel_74365/3678118878.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize the model with class_weight='balanced' for imbalance\u001b[39;00m\n\u001b[32m      5\u001b[39m logreg = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m, class_weight=\u001b[33m'balanced'\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m logreg.fit(X_train, y_train)\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m     11\u001b[39m y_pred = logreg.predict(X_val)\n",
      "\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1385\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m                 )\n\u001b[32m   1388\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1218\u001b[39m             _dtype = np.float64\n\u001b[32m   1219\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m             _dtype = [np.float64, np.float32]\n\u001b[32m   1221\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m         X, y = validate_data(\n\u001b[32m   1223\u001b[39m             self,\n\u001b[32m   1224\u001b[39m             X,\n\u001b[32m   1225\u001b[39m             y,\n",
      "\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2957\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"estimator\"\u001b[39m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m check_y_params:\n\u001b[32m   2958\u001b[39m                 check_y_params = {**default_check_params, **check_y_params}\n\u001b[32m   2959\u001b[39m             y = check_array(y, input_name=\u001b[33m\"y\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m             X, y = check_X_y(X, y, **check_params)\n\u001b[32m   2962\u001b[39m         out = X, y\n\u001b[32m   2963\u001b[39m \n\u001b[32m   2964\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m check_params.get(\u001b[33m\"ensure_2d\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1366\u001b[39m         )\n\u001b[32m   1367\u001b[39m \n\u001b[32m   1368\u001b[39m     ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m   1369\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m     X = check_array(\n\u001b[32m   1371\u001b[39m         X,\n\u001b[32m   1372\u001b[39m         accept_sparse=accept_sparse,\n\u001b[32m   1373\u001b[39m         accept_large_sparse=accept_large_sparse,\n",
      "\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1052\u001b[39m                         )\n\u001b[32m   1053\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1055\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m                 raise ValueError(\n\u001b[32m   1058\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    835\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    836\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    837\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    840\u001b[39m \n\u001b[32m    841\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2149\u001b[39m     def __array__(\n\u001b[32m   2150\u001b[39m         self, dtype: npt.DTypeLike | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, copy: bool_t | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2151\u001b[39m     ) -> np.ndarray:\n\u001b[32m   2152\u001b[39m         values = self._values\n\u001b[32m-> \u001b[39m\u001b[32m2153\u001b[39m         arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2154\u001b[39m         if (\n\u001b[32m   2155\u001b[39m             astype_is_view(values.dtype, arr.dtype)\n\u001b[32m   2156\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m using_copy_on_write()\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '05Jan2017'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns in X_train:\n",
      "['Var13']\n"
     ]
    }
   ],
   "source": [
    "# Check data types in X_train\n",
    "print(\"Non-numeric columns in X_train:\")\n",
    "non_numeric_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(non_numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime and extract year/month if needed\n",
    "X_train['Var13'] = pd.to_datetime(X_train['Var13'], format='%d%b%Y', errors='coerce')\n",
    "X_val['Var13'] = pd.to_datetime(X_val['Var13'], format='%d%b%Y', errors='coerce')\n",
    "\n",
    "# Extract year as a numeric feature\n",
    "X_train['Var13_year'] = X_train['Var13'].dt.year\n",
    "X_val['Var13_year'] = X_val['Var13'].dt.year\n",
    "\n",
    "# Optionally drop the raw string column\n",
    "X_train = X_train.drop(columns=['Var13'])\n",
    "X_val = X_val.drop(columns=['Var13'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any remaining non-numeric columns? []\n"
     ]
    }
   ],
   "source": [
    "print(\"Any remaining non-numeric columns?\", X_train.select_dtypes(include=['object', 'category']).columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDTypePromotionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m logreg = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m, class_weight=\u001b[33m'\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mlogreg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m     11\u001b[39m y_pred = logreg.predict(X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1222\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m check_classification_targets(y)\n\u001b[32m   1232\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:931\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    927\u001b[39m pandas_requires_conversion = \u001b[38;5;28many\u001b[39m(\n\u001b[32m    928\u001b[39m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[32m    929\u001b[39m )\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np.dtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m     dtype_orig = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m    933\u001b[39m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[32m    934\u001b[39m     dtype_orig = \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[31mDTypePromotionError\u001b[39m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int32DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>)"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Initialize the model with class_weight='balanced' for imbalance\n",
    "logreg = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Train\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(X_val)\n",
    "y_proba = logreg.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"=== Logistic Regression Performance ===\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, y_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datetime columns in X_train:\n",
      "['Var13_date']\n"
     ]
    }
   ],
   "source": [
    "print(\"Datetime columns in X_train:\")\n",
    "date_cols = X_train.select_dtypes(include=['datetime']).columns.tolist()\n",
    "print(date_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the datetime column\n",
    "X_train = X_train.drop(columns=['Var13_date'])\n",
    "X_val = X_val.drop(columns=['Var13_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any remaining non-numeric columns? []\n"
     ]
    }
   ],
   "source": [
    "print(\"Any remaining non-numeric columns?\", X_train.select_dtypes(include=['object', 'category', 'datetime']).columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlogreg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1222\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m check_classification_targets(y)\n\u001b[32m   1232\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X_train:\n",
      "Var2             797\n",
      "Var17             27\n",
      "Var25           5916\n",
      "Var26          11697\n",
      "Var13_year       468\n",
      "Var13_month      468\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in X_train:\")\n",
    "print(X_train.isnull().sum()[X_train.isnull().sum() > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After imputation - any NaNs left in X_train? 0\n",
      "After imputation - any NaNs left in X_val? 0\n"
     ]
    }
   ],
   "source": [
    "# Fill NaNs in both train and val with train median\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_val = X_val.fillna(X_train.median())\n",
    "\n",
    "print(\"After imputation - any NaNs left in X_train?\", X_train.isnull().sum().sum())\n",
    "print(\"After imputation - any NaNs left in X_val?\", X_val.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(class_weight=&#x27;balanced&#x27;, max_iter=1000, random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "    max_iter=5000,  # 🚀 Increase this\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9768    0.9576    0.9671      7119\n",
      "         1.0     0.1726    0.2800    0.2136       225\n",
      "\n",
      "    accuracy                         0.9368      7344\n",
      "   macro avg     0.5747    0.6188    0.5903      7344\n",
      "weighted avg     0.9521    0.9368    0.9440      7344\n",
      "\n",
      "ROC-AUC: 0.7589474177084795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf.predict(X_val)\n",
    "y_proba_rf = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"\\n=== Random Forest Performance ===\")\n",
    "print(classification_report(y_val, y_pred_rf, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, y_proba_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature importances from Random Forest\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "\n",
    "# Sort and plot top 20\n",
    "top_features = feature_importances.sort_values(ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features.plot(kind='barh')\n",
    "plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.4926 with F1: 0.2188\n",
      "=== Random Forest Performance (Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9776    0.9515    0.9644      7119\n",
      "         1.0     0.1687    0.3111    0.2188       225\n",
      "\n",
      "    accuracy                         0.9319      7344\n",
      "   macro avg     0.5732    0.6313    0.5916      7344\n",
      "weighted avg     0.9528    0.9319    0.9416      7344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# Get precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_proba_rf)\n",
    "\n",
    "# Compute F1 scores\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Find best threshold\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"Best threshold: {best_threshold:.4f} with F1: {f1_scores[best_idx]:.4f}\")\n",
    "\n",
    "# Predict with new threshold\n",
    "y_pred_opt = (y_proba_rf >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"=== Random Forest Performance (Optimized Threshold) ===\")\n",
    "print(classification_report(y_val, y_pred_opt, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set shape: (29374, 40)\n",
      "Resampled training set shape: (56944, 40)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit and resample\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Original training set shape:\", X_train.shape)\n",
    "print(\"Resampled training set shape:\", X_train_resampled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest Performance (SMOTE Resampled) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9764    0.8932    0.9330      7119\n",
      "         1.0     0.0854    0.3156    0.1345       225\n",
      "\n",
      "    accuracy                         0.8755      7344\n",
      "   macro avg     0.5309    0.6044    0.5337      7344\n",
      "weighted avg     0.9491    0.8755    0.9085      7344\n",
      "\n",
      "ROC-AUC: 0.6504864915484385\n"
     ]
    }
   ],
   "source": [
    "rf_resampled = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_resampled.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf_resampled = rf_resampled.predict(X_val)\n",
    "y_proba_rf_resampled = rf_resampled.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n=== Random Forest Performance (SMOTE Resampled) ===\")\n",
    "print(classification_report(y_val, y_pred_rf_resampled, digits=4))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, y_proba_rf_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  18.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  18.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  19.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  19.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=  19.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=  18.9s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  17.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  17.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=  17.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  23.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  23.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.2s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=500; total time=   0.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=  23.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  15.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  15.7s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=2, min_samples_split=10, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=  15.0s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  51.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 1.2min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  53.2s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.4s\n",
      "[CV] END max_depth=20, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=100; total time=   0.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=  52.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  31.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  40.9s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  41.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  32.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=  41.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=  32.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time= 1.3min\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=4, min_samples_split=10, n_estimators=200; total time=   0.1s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.3s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time=   0.5s\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.9min\n",
      "[CV] END max_depth=None, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=4, min_samples_split=10, n_estimators=500; total time= 1.8min\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=500; total time=  33.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
      "24 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.96740529 0.96636779 0.96923922        nan        nan 0.96618688\n",
      " 0.9959114  0.99520511        nan 0.96898484        nan        nan\n",
      " 0.99607578 0.96588805        nan 0.99815927 0.99672669 0.96763033\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n",
      "Best ROC-AUC (CV): 0.9981592731829805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [5, 10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run search on resampled data (best practice)\n",
    "rf_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"Best parameters:\", rf_search.best_params_)\n",
    "print(\"Best ROC-AUC (CV):\", rf_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'log2',\n",
       " 'max_depth': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    'n_estimators': 500,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'log2',\n",
    "    'max_depth': None\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Best Random Forest Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9707    0.9975    0.9839      7119\n",
      "         1.0     0.3793    0.0489    0.0866       225\n",
      "\n",
      "    accuracy                         0.9684      7344\n",
      "   macro avg     0.6750    0.5232    0.5353      7344\n",
      "weighted avg     0.9526    0.9684    0.9564      7344\n",
      "\n",
      "ROC-AUC: 0.6943134959654134\n"
     ]
    }
   ],
   "source": [
    "# Train the optimized Random Forest\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='log2',\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "best_rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict\n",
    "y_pred_best = best_rf.predict(X_val)\n",
    "y_proba_best = best_rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"\\n=== Best Random Forest Performance ===\")\n",
    "print(classification_report(y_val, y_pred_best, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, y_proba_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9713    0.9985    0.9847      7119\n",
      "         1.0     0.5769    0.0667    0.1195       225\n",
      "\n",
      "    accuracy                         0.9699      7344\n",
      "   macro avg     0.7741    0.5326    0.5521      7344\n",
      "weighted avg     0.9592    0.9699    0.9582      7344\n",
      "\n",
      "ROC-AUC: 0.7278690827363394\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=(len(y_train_resampled[y_train_resampled == 0]) / len(y_train_resampled[y_train_resampled == 1])),\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "y_proba_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(\"\\n=== XGBoost Performance ===\")\n",
    "print(classification_report(y_val, y_pred_xgb, digits=4))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, y_proba_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Blended Model Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9712    0.9989    0.9848      7119\n",
      "         1.0     0.6364    0.0622    0.1134       225\n",
      "\n",
      "    accuracy                         0.9702      7344\n",
      "   macro avg     0.8038    0.5305    0.5491      7344\n",
      "weighted avg     0.9609    0.9702    0.9581      7344\n",
      "\n",
      "ROC-AUC: 0.7216200777262723\n"
     ]
    }
   ],
   "source": [
    "# Average probabilities\n",
    "avg_proba = (y_proba_best + y_proba_xgb) / 2\n",
    "\n",
    "# Threshold 0.5 (or tune as before)\n",
    "y_pred_blend = (avg_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== Blended Model Performance ===\")\n",
    "print(classification_report(y_val, y_pred_blend, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, avg_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.1107 with F1: 0.2397\n",
      "\n",
      "=== XGBoost Performance (Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9768    0.9687    0.9727      7119\n",
      "         1.0     0.2148    0.2711    0.2397       225\n",
      "\n",
      "    accuracy                         0.9473      7344\n",
      "   macro avg     0.5958    0.6199    0.6062      7344\n",
      "weighted avg     0.9534    0.9473    0.9502      7344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_proba_xgb)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"Best threshold: {best_threshold:.4f} with F1: {f1_scores[best_idx]:.4f}\")\n",
    "\n",
    "# Predict with optimized threshold\n",
    "y_pred_xgb_opt = (y_proba_xgb >= best_threshold).astype(int)\n",
    "print(\"\\n=== XGBoost Performance (Optimized Threshold) ===\")\n",
    "print(classification_report(y_val, y_pred_xgb_opt, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.2100 with F1: 0.2287\n",
      "\n",
      "=== Blended Model Performance (Optimized Threshold) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9751    0.9805    0.9778      7119\n",
      "         1.0     0.2527    0.2089    0.2287       225\n",
      "\n",
      "    accuracy                         0.9568      7344\n",
      "   macro avg     0.6139    0.5947    0.6033      7344\n",
      "weighted avg     0.9530    0.9568    0.9548      7344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, avg_proba_blend)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"Best threshold: {best_threshold:.4f} with F1: {f1_scores[best_idx]:.4f}\")\n",
    "\n",
    "# Predict with optimized threshold\n",
    "y_pred_blend_opt = (avg_proba_blend >= best_threshold).astype(int)\n",
    "print(\"\\n=== Blended Model Performance (Optimized Threshold) ===\")\n",
    "print(classification_report(y_val, y_pred_blend_opt, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Blended Model Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9712    0.9989    0.9848      7119\n",
      "         1.0     0.6364    0.0622    0.1134       225\n",
      "\n",
      "    accuracy                         0.9702      7344\n",
      "   macro avg     0.8038    0.5305    0.5491      7344\n",
      "weighted avg     0.9609    0.9702    0.9581      7344\n",
      "\n",
      "ROC-AUC: 0.7216200777262723\n"
     ]
    }
   ],
   "source": [
    "# Average probabilities\n",
    "avg_proba_blend = (y_proba_xgb + y_proba_best) / 2\n",
    "\n",
    "# Threshold 0.5 (or tune)\n",
    "y_pred_blend = (avg_proba_blend >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== Blended Model Performance ===\")\n",
    "print(classification_report(y_val, y_pred_blend, digits=4))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, avg_proba_blend))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EasyEnsemble Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9852    0.7678    0.8630      7119\n",
      "         1.0     0.0796    0.6356    0.1415       225\n",
      "\n",
      "    accuracy                         0.7638      7344\n",
      "   macro avg     0.5324    0.7017    0.5023      7344\n",
      "weighted avg     0.9575    0.7638    0.8409      7344\n",
      "\n",
      "ROC-AUC: 0.7705910630394407\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "eec = EasyEnsembleClassifier(\n",
    "    n_estimators=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "eec.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_eec = eec.predict(X_val)\n",
    "y_proba_eec = eec.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n=== EasyEnsemble Performance ===\")\n",
    "print(classification_report(y_val, y_pred_eec, digits=4))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, y_proba_eec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Balanced Bagging Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9822    0.8897    0.9337      7119\n",
      "         1.0     0.1229    0.4889    0.1964       225\n",
      "\n",
      "    accuracy                         0.8775      7344\n",
      "   macro avg     0.5525    0.6893    0.5650      7344\n",
      "weighted avg     0.9558    0.8775    0.9111      7344\n",
      "\n",
      "ROC-AUC: 0.7761208659143762\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bbc = BalancedBaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "bbc.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_bbc = bbc.predict(X_val)\n",
    "y_proba_bbc = bbc.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(\"\\n=== Balanced Bagging Performance ===\")\n",
    "print(classification_report(y_val, y_pred_bbc, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, y_proba_bbc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stacking Classifier Performance ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9715    0.9990    0.9850      7119\n",
      "         1.0     0.6957    0.0711    0.1290       225\n",
      "\n",
      "    accuracy                         0.9706      7344\n",
      "   macro avg     0.8336    0.5351    0.5570      7344\n",
      "weighted avg     0.9630    0.9706    0.9588      7344\n",
      "\n",
      "ROC-AUC: 0.7742373304615191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, random_state=42)),\n",
    "    ('xgb', xgb.XGBClassifier(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "]\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train\n",
    "stack.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_stack = stack.predict(X_val)\n",
    "y_proba_stack = stack.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n=== Stacking Classifier Performance ===\")\n",
    "print(classification_report(y_val, y_pred_stack, digits=4))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_val, y_proba_stack))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature check debt/income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df['Var5_Var10_ratio'] = dev_df['Var5'] / (dev_df['Var10'] + 1e-6)\n",
    "test_df['Var5_Var10_ratio'] = test_df['Var5'] / (test_df['Var10'] + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EasyEnsemble PR-AUC: 0.2172\n",
      "Balanced Bagging PR-AUC: 0.1627\n",
      "Stacking Classifier PR-AUC: 0.2211\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "pr_auc_easy = average_precision_score(y_val, y_proba_eec)\n",
    "print(f\"EasyEnsemble PR-AUC: {pr_auc_easy:.4f}\")\n",
    "\n",
    "\n",
    "pr_auc_bbc = average_precision_score(y_val, y_proba_bbc)\n",
    "print(f\"Balanced Bagging PR-AUC: {pr_auc_bbc:.4f}\")\n",
    "\n",
    "pr_auc_stack = average_precision_score(y_val, y_proba_stack)\n",
    "print(f\"Stacking Classifier PR-AUC: {pr_auc_stack:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest ROC-AUC: 0.7773 | PR-AUC: 0.1934\n",
      "Best threshold: 0.0476 with F1: 0.2697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9783    0.9667    0.9724      7119\n",
      "         1.0     0.2330    0.3200    0.2697       225\n",
      "\n",
      "    accuracy                         0.9469      7344\n",
      "   macro avg     0.6056    0.6434    0.6211      7344\n",
      "weighted avg     0.9554    0.9469    0.9509      7344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Base RandomForest\n",
    "rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "rf_cal = CalibratedClassifierCV(rf, cv=3)\n",
    "\n",
    "# Train\n",
    "rf_cal.fit(X_train, y_train)\n",
    "\n",
    "# Predict probs\n",
    "y_proba_rf = rf_cal.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# ROC-AUC + PR-AUC\n",
    "roc_rf = roc_auc_score(y_val, y_proba_rf)\n",
    "pr_auc_rf = average_precision_score(y_val, y_proba_rf)\n",
    "print(f\"Random Forest ROC-AUC: {roc_rf:.4f} | PR-AUC: {pr_auc_rf:.4f}\")\n",
    "\n",
    "# Threshold tuning\n",
    "prec, rec, thresh = precision_recall_curve(y_val, y_proba_rf)\n",
    "f1_scores = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresh[np.argmax(f1_scores)]\n",
    "print(f\"Best threshold: {best_thresh:.4f} with F1: {np.max(f1_scores):.4f}\")\n",
    "\n",
    "# Final prediction\n",
    "y_pred_rf = (y_proba_rf >= best_thresh).astype(int)\n",
    "print(classification_report(y_val, y_pred_rf, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost ROC-AUC: 0.7237 | PR-AUC: 0.1800\n",
      "Best threshold: 0.2702 with F1: 0.2440\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9746    0.9902    0.9823      7119\n",
      "         1.0     0.3694    0.1822    0.2440       225\n",
      "\n",
      "    accuracy                         0.9654      7344\n",
      "   macro avg     0.6720    0.5862    0.6132      7344\n",
      "weighted avg     0.9560    0.9654    0.9597      7344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=200, scale_pos_weight=5, random_state=42, n_jobs=-1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_proba_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "roc_xgb = roc_auc_score(y_val, y_proba_xgb)\n",
    "pr_auc_xgb = average_precision_score(y_val, y_proba_xgb)\n",
    "print(f\"XGBoost ROC-AUC: {roc_xgb:.4f} | PR-AUC: {pr_auc_xgb:.4f}\")\n",
    "\n",
    "prec, rec, thresh = precision_recall_curve(y_val, y_proba_xgb)\n",
    "f1_scores = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresh[np.argmax(f1_scores)]\n",
    "print(f\"Best threshold: {best_thresh:.4f} with F1: {np.max(f1_scores):.4f}\")\n",
    "\n",
    "y_pred_xgb = (y_proba_xgb >= best_thresh).astype(int)\n",
    "print(classification_report(y_val, y_pred_xgb, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression ROC-AUC: 0.7324 | PR-AUC: 0.1554\n",
      "Best threshold: 0.7515 with F1: 0.2134\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9746    0.9810    0.9778      7119\n",
      "         1.0     0.2416    0.1911    0.2134       225\n",
      "\n",
      "    accuracy                         0.9568      7344\n",
      "   macro avg     0.6081    0.5861    0.5956      7344\n",
      "weighted avg     0.9521    0.9568    0.9544      7344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshma/Documents/DS/semester4/ubproject/UBproject/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_proba_log = logreg.predict_proba(X_val)[:, 1]\n",
    "\n",
    "roc_log = roc_auc_score(y_val, y_proba_log)\n",
    "pr_auc_log = average_precision_score(y_val, y_proba_log)\n",
    "print(f\"Logistic Regression ROC-AUC: {roc_log:.4f} | PR-AUC: {pr_auc_log:.4f}\")\n",
    "\n",
    "prec, rec, thresh = precision_recall_curve(y_val, y_proba_log)\n",
    "f1_scores = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresh[np.argmax(f1_scores)]\n",
    "print(f\"Best threshold: {best_thresh:.4f} with F1: {np.max(f1_scores):.4f}\")\n",
    "\n",
    "y_pred_log = (y_proba_log >= best_thresh).astype(int)\n",
    "print(classification_report(y_val, y_pred_log, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape: Counter({0.0: 28407, 1.0: 28407})\n",
      "Random Forest (SMOTE + Tomek) ROC-AUC: 0.6960 | PR-AUC: 0.1279\n",
      "Best threshold: 0.3050 with F1: 0.1805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9735    0.9819    0.9777      7119\n",
      "         1.0     0.2134    0.1556    0.1799       225\n",
      "\n",
      "    accuracy                         0.9566      7344\n",
      "   macro avg     0.5935    0.5687    0.5788      7344\n",
      "weighted avg     0.9502    0.9566    0.9533      7344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "# Resample\n",
    "smt = SMOTETomek(random_state=42)\n",
    "X_resampled, y_resampled = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Resampled dataset shape: {Counter(y_resampled)}\")\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)\n",
    "rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict probs\n",
    "y_proba_rf = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_val, y_proba_rf)\n",
    "pr_auc = average_precision_score(y_val, y_proba_rf)\n",
    "print(f\"Random Forest (SMOTE + Tomek) ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Best threshold\n",
    "prec, rec, thresh = precision_recall_curve(y_val, y_proba_rf)\n",
    "f1_scores = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresh[np.argmax(f1_scores)]\n",
    "print(f\"Best threshold: {best_thresh:.4f} with F1: {np.max(f1_scores):.4f}\")\n",
    "\n",
    "# Final prediction\n",
    "y_pred_rf = (y_proba_rf > best_thresh).astype(int)\n",
    "print(classification_report(y_val, y_pred_rf, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 28407, number of negative: 28407\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011558 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4559\n",
      "[LightGBM] [Info] Number of data points in the train set: 56814, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LightGBM ROC-AUC: 0.7210 | PR-AUC: 0.1800\n",
      "Best threshold: 0.0671 with F1: 0.2242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9767    0.9615    0.9691      7119\n",
      "         1.0     0.1845    0.2756    0.2210       225\n",
      "\n",
      "    accuracy                         0.9405      7344\n",
      "   macro avg     0.5806    0.6185    0.5951      7344\n",
      "weighted avg     0.9525    0.9405    0.9461      7344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# LGBM Model\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgbm.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict\n",
    "y_proba_lgb = lgbm.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_val, y_proba_lgb)\n",
    "pr_auc = average_precision_score(y_val, y_proba_lgb)\n",
    "print(f\"LightGBM ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Best threshold\n",
    "prec, rec, thresh = precision_recall_curve(y_val, y_proba_lgb)\n",
    "f1_scores = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresh[np.argmax(f1_scores)]\n",
    "print(f\"Best threshold: {best_thresh:.4f} with F1: {np.max(f1_scores):.4f}\")\n",
    "\n",
    "# Final prediction\n",
    "y_pred_lgb = (y_proba_lgb > best_thresh).astype(int)\n",
    "print(classification_report(y_val, y_pred_lgb, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006228 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=31, scale_pos_weight=10; total time=  10.6s\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=31, scale_pos_weight=10; total time=  11.4s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=200, num_leaves=31, scale_pos_weight=10; total time=   9.8s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=1000, num_leaves=50, scale_pos_weight=5; total time=  43.6s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004352 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=70, scale_pos_weight=1; total time=  46.2s\n",
      "[CV] END learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=70, scale_pos_weight=1; total time=  46.2s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.01, max_depth=-1, n_estimators=500, num_leaves=70, scale_pos_weight=1; total time=  46.7s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=70, scale_pos_weight=1; total time=  43.9s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=70, scale_pos_weight=1; total time=  36.3s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=1000, num_leaves=50, scale_pos_weight=5; total time= 1.0min\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=70, scale_pos_weight=5; total time=  15.9s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=70, scale_pos_weight=5; total time=  16.3s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.05, max_depth=20, n_estimators=200, num_leaves=70, scale_pos_weight=5; total time=  16.1s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.1, max_depth=-1, n_estimators=1000, num_leaves=50, scale_pos_weight=5; total time= 1.3min\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.05, max_depth=10, n_estimators=500, num_leaves=70, scale_pos_weight=1; total time=  37.0s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003997 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=70, scale_pos_weight=1; total time=  39.0s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=70, scale_pos_weight=1; total time=  38.0s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END learning_rate=0.01, max_depth=10, n_estimators=500, num_leaves=70, scale_pos_weight=1; total time=  45.2s\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=70, scale_pos_weight=10; total time=  35.5s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=70, scale_pos_weight=5; total time=  16.5s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4550\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=70, scale_pos_weight=5; total time=  16.9s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4543\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=70, scale_pos_weight=10; total time=  40.0s\n",
      "[LightGBM] [Info] Number of positive: 18938, number of negative: 18938\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4534\n",
      "[LightGBM] [Info] Number of data points in the train set: 37876, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[CV] END learning_rate=0.01, max_depth=-1, n_estimators=200, num_leaves=70, scale_pos_weight=5; total time=  16.0s\n",
      "[CV] END learning_rate=0.1, max_depth=20, n_estimators=500, num_leaves=70, scale_pos_weight=10; total time=  39.3s\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=1000, num_leaves=70, scale_pos_weight=10; total time= 1.4min\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=1000, num_leaves=70, scale_pos_weight=10; total time= 1.4min\n",
      "[CV] END learning_rate=0.01, max_depth=20, n_estimators=1000, num_leaves=70, scale_pos_weight=10; total time= 1.4min\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV] END learning_rate=0.05, max_depth=20, n_estimators=1000, num_leaves=50, scale_pos_weight=5; total time=  39.2s\n",
      "[CV] END learning_rate=0.05, max_depth=20, n_estimators=1000, num_leaves=50, scale_pos_weight=5; total time=  36.0s\n",
      "[CV] END learning_rate=0.05, max_depth=20, n_estimators=1000, num_leaves=50, scale_pos_weight=5; total time=  37.8s\n",
      "[LightGBM] [Info] Number of positive: 28407, number of negative: 28407\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4559\n",
      "[LightGBM] [Info] Number of data points in the train set: 56814, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best params: {'scale_pos_weight': 10, 'num_leaves': 70, 'n_estimators': 500, 'max_depth': 20, 'learning_rate': 0.1}\n",
      "Best PR-AUC (CV): 0.9962910565694347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [200, 500, 1000],\n",
    "    'scale_pos_weight': [1, 5, 10]\n",
    "}\n",
    "\n",
    "lgbm = lgb.LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "grid = RandomizedSearchCV(lgbm, param_distributions=param_grid, scoring='average_precision', n_iter=10, cv=3, n_jobs=-1, verbose=2)\n",
    "grid.fit(X_resampled, y_resampled)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best PR-AUC (CV):\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 28407, number of negative: 28407\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4559\n",
      "[LightGBM] [Info] Number of data points in the train set: 56814, number of used features: 39\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "LightGBM (tuned) ROC-AUC: 0.7155 | PR-AUC: 0.1771\n",
      "Best threshold: 0.1966 with F1: 0.2173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9737    0.9924    0.9830      7119\n",
      "         1.0     0.3864    0.1511    0.2173       225\n",
      "\n",
      "    accuracy                         0.9666      7344\n",
      "   macro avg     0.6800    0.5718    0.6001      7344\n",
      "weighted avg     0.9557    0.9666    0.9595      7344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    average_precision_score, \n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "import seaborn as sns\n",
    "# best_params \n",
    "best_params = {\n",
    "    'scale_pos_weight': 10,\n",
    "    'num_leaves': 70,\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 20,\n",
    "    'learning_rate': 0.1\n",
    "}\n",
    "\n",
    "# Fit LightGBM with best params\n",
    "best_lgbm = lgb.LGBMClassifier(\n",
    "    **best_params, \n",
    "    random_state=42\n",
    ")\n",
    "best_lgbm.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Predict probabilities on validation set\n",
    "y_proba_lgbm = best_lgbm.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# ROC-AUC & PR-AUC\n",
    "roc_auc = roc_auc_score(y_val, y_proba_lgbm)\n",
    "pr_auc = average_precision_score(y_val, y_proba_lgbm)\n",
    "print(f\"\\nLightGBM (tuned) ROC-AUC: {roc_auc:.4f} | PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "# Threshold optimization for best F1\n",
    "prec, rec, thresh = precision_recall_curve(y_val, y_proba_lgbm)\n",
    "f1_scores = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
    "best_thresh = thresh[np.argmax(f1_scores)]\n",
    "print(f\"Best threshold: {best_thresh:.4f} with F1: {np.max(f1_scores):.4f}\")\n",
    "\n",
    "# Final prediction using best threshold\n",
    "y_pred_lgbm = (y_proba_lgbm >= best_thresh).astype(int)\n",
    "print(classification_report(y_val, y_pred_lgbm, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
